[main]
testTopDir     = /home/regtester/AMReX_RegTesting/rt-incflo/
webTopDir      = /home/regtester/AMReX_RegTesting/rt-incflo/web

useCmake = 0

sourceTree = C_Src

# suiteName is the name prepended to all output directories
suiteName = incflo

COMP = g++
FCOMP = gfortran
add_to_c_make_command = TEST=TRUE USE_ASSERTION=TRUE

purge_output = 1

MAKE = make
numMakeJobs = 8

# MPIcommand should use the placeholders:
#   @host@ to indicate where to put the hostname to run on
#   @nprocs@ to indicate where to put the number of processors
#   @command@ to indicate where to put the command to run
#
# only tests with useMPI = 1 will run in parallel
# nprocs is problem dependent and specified in the individual problem
# sections.

#MPIcommand = mpiexec -host @host@ -n @nprocs@ @command@
MPIcommand = mpiexec -n @nprocs@ @command@
MPIhost = 

reportActiveTestsOnly = 1

# Add "GO UP" link at the top of the web page?
goUpLink = 1

# email
sendEmailWhenFail = 1
emailTo = ksk38@cam.ac.uk
emailBody = Check https://ccse.lbl.gov/pub/RegressionTesting/incflo/

[AMReX]
dir = /home/regtester/AMReX_RegTesting/amrex
branch = development

[source]
dir = /home/regtester/AMReX_RegTesting/incflo
branch = development

# individual problems follow

[double_shear_layer] 
buildDir = test
inputFile = benchmark.double_shear_layer
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0

[taylor_green_vortices] 
buildDir = test
inputFile = benchmark.taylor_green_vortices
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0

[couette] 
buildDir = test
inputFile = benchmark.couette
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0

[couette_poiseuille] 
buildDir = test
inputFile = benchmark.couette_poiseuille
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0

[lid_driven_cavity] 
buildDir = test
inputFile = benchmark.lid_driven_cavity
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0

[poiseuille_plane_newtonian] 
buildDir = test
inputFile = benchmark.poiseuille_plane_newtonian
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0

[poiseuille_plane_bingham] 
buildDir = test
inputFile = benchmark.poiseuille_plane_bingham
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0

[poiseuille_cylinder_newtonian] 
buildDir = test
inputFile = benchmark.poiseuille_cylinder_newtonian
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0

[poiseuille_cylinder_bingham] 
buildDir = test
inputFile = benchmark.poiseuille_cylinder_bingham
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0

[uniform_velocity_sphere]
buildDir = test
inputFile = benchmark.uniform_velocity_sphere
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0

[channel_cylinder]
buildDir = test
inputFile = benchmark.channel_cylinder
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0

[channel_spherecube]
buildDir = test
inputFile = benchmark.channel_spherecube
target = incflo
dim = 3
restartTest = 0
useMPI = 1
numprocs = 8
compileTest = 0
doVis = 0



